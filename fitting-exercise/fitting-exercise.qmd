---
title: "Fitting Exercise"
editor: 
  markdown: 
    wrap: 72
editor_options: 
  chunk_output_type: console
---

# Introduction

For this assignment, I will be using data on a drug candidate called
Mavoglurant from the paper:

Wendling, T., Dumitras, S., Ogungbenro, K. et al. Application of a
Bayesian approach to physiological modelling of mavoglurant population
pharmacokinetics. J Pharmacokinet Pharmacodyn 42, 639â€“657 (2015).
https://doi.org/10.1007/s10928-015-9430-4

## Loading

First we will load the data and additional packages necessary for the
assigniment.

```{r}
library(readr) #for loading Excel files
library(dplyr) #for data processing/cleaning
library(tidyr) #for data processing/cleaning
library(skimr) #for nice visualization of data 
library(here) #to set paths
library(ggplot2) # for plots
library(gtsummary)# for summary tables
library(patchwork) #for combine plots
library(tidymodels)
```

I will also load the data from the another paper which has used this
data to ensure a level of consistency and ease of usage with the data.

```{r}
# Loading Data through CSV
mavodrug <-- read.csv('Mavoglurant_A2121_nmpk.csv')
mavodrug <- abs(mavodrug)
```

# Exploring and Processing

```{r}
#summary(rawdata)
skimr::skim(mavodrug)
```

This is time-series data of drug concentrations. So for further
exploration, we will consider time as the independent variable. To
visualilze the data, We will plot the outcome variable (DV) as a
function of time, stratified by DOSE and using ID as a grouping factor.

```{r}
# Plotting Outcome Variable

# Plotting Outcome Variable
mavoplot1<- mavodrug %>% ggplot()+
      geom_line(aes(x= TIME, y=DV, group = as.factor(ID), color= as.factor(DOSE))) +
      facet_wrap(~DOSE, scales = "free_y")
mavoplot1
```

There are some individuals that have received the drug more than once,
indicated by having both entries with OCC=1 and OCC=2. To keep things
simple, I only keep one dataset for each individual and remove all
entries with OCC=2.

```{r}
# Checking unique values in the 'OCC' column
unique(mavodrug$OCC)

# Filter observations with OCC = 1
mavodrug_clean <- mavodrug[mavodrug$OCC == 1, ]

# Check the structure of the data frame after filtering
str(mavodrug_clean)

mavoplot1.1<- mavodrug_clean %>% ggplot()+
    geom_line(aes(x= TIME, y=DV, group = as.factor(ID), color= as.factor(DOSE))) +
      facet_wrap(~DOSE, scales = "free_y")
mavoplot1.1
```

Here, I will compute the total amount of drug for each individual (for
exercise purposes only, not used in normal practices).

```{r}
# Exclude observations with TIME = 0
mavodrug_filtered <- mavodrug_clean %>%
  filter(TIME != 0)

# Compute the sum of the DV variable for each individual
summarized_mavo <- mavodrug_filtered %>%
  group_by(ID) %>%
  summarize(Y = sum(DV))

# Create a data frame containing only the observations where TIME == 0
time_0_mavo <- mavodrug_clean %>%
  filter(TIME == 0)

# Use the appropriate join function to combine the two data frames
combined_mavo <- inner_join(summarized_mavo, time_0_mavo, by = "ID")

# Check the dimensions of the resulting data frame
dim(time_0_mavo)
dim(combined_mavo)
```

```{r}
# Select only the specified variables and check the structure
cleaned_mavo <- combined_mavo %>%
  select(Y, DOSE, AGE, SEX, WT, HT) %>%
  #converting SEX as factors
  mutate(across(c(SEX), as.factor))

readr::write_rds(cleaned_mavo, "mavoglurant.rds")

# Check the structure of the cleaned data
skimr::skim(cleaned_mavo)
```

## Summary Tables

The summary tables provide a quick overview of the numerical and
categorical variables in the dataset.

```{r}
mavotable1 <- tbl_summary(
  cleaned_mavo,
  by = DOSE, # Stratify summary by DOSE
  type = list(
    DOSE ~ "categorical", #Specifying DOSE as categorical
    SEX ~ "categorical",  #Specifying SES as categorical
    Y ~ "continuous2",
    AGE ~ "continuous2",
    WT ~ "continuous2",
    HT ~ "continuous2"
  ),
  statistic = list(
    all_continuous() ~ c("{mean} ({sd})", "{min}, {max}"), # Statistics for continuous variables
    all_categorical() ~ "{n} ({p}%)"), # Statistics for categorical variables
  missing = "no" # Option to exclude missing data in summary
)

mavotable1
```

## Plots

These plots show the relationship between the total drug ('Y') and other
predictors such as age, dose, and sex. Scatterplots help visualize the
continuous relationship between 'Y' and age, while boxplots illustrate
the distribution of 'Y' across different levels of dose and sex.

```{r}
mavoplot2 <- cleaned_mavo %>%
  mutate(DOSE = as.factor(DOSE)) %>% # Convert DOSE to factor here
  ggplot(aes(x = AGE, y = Y, group = DOSE, col = DOSE)) +
  geom_point() + 
  geom_smooth(method = lm, se = FALSE) + # Add a linear regression line
  scale_color_viridis_d(option = "plasma", end = .7)+
  labs(title = "Scatterplot of Y vs. AGE", x = "Age", y = "Total Drug (Y)") +
  theme_minimal()
mavoplot2

mavoplot3 <- cleaned_mavo %>%
  mutate(DOSE = as.factor(DOSE)) %>% # Convert DOSE to factor here
  ggplot(aes(x = HT, y = Y, group = DOSE, col = DOSE)) +
  geom_point() + 
  geom_smooth(method = lm, se = FALSE) + # Add a linear regression line
  scale_color_viridis_d(option = "inferno", end = .7)+
  labs(title = "Scatterplot of Y vs. HT", x = "HT", y = "Total Drug (Y)") +
  theme_minimal()
mavoplot3

mavoplot4<-cleaned_mavo %>%
  mutate(DOSE = as.factor(DOSE)) %>% # Convert DOSE to factor here
  ggplot(aes(x = WT, y = Y, group = DOSE, col = DOSE)) +
  geom_point() + 
  geom_smooth(method = lm, se = FALSE) + # Add a linear regression line
  scale_color_viridis_d(option = "magma", end = .7)+
  labs(title = "Scatterplot of Y vs. WT", x = "WT", y = "Total Drug (Y)") +
  theme_minimal()
mavoplot4

mavoplot5 <- cleaned_mavo %>%
  mutate(DOSE = as.factor(DOSE)) %>% # Converting DOSE to factor only for the plot
  ggplot(aes(x = DOSE, y = Y, fill = DOSE)) + 
  geom_boxplot() + 
  labs(title = "Distribution of Y by DOSE", x = "DOSE", y = "Y") +
  theme_minimal()
mavoplot5

mavoplot6 <- cleaned_mavo %>%
  mutate(DOSE = as.factor(DOSE)) %>% # Convert DOSE to factor here
  select(DOSE, SEX) %>%
  pivot_longer(everything(), names_to = "name", values_to = "value") %>%
  mutate(name = factor(name, levels = c("DOSE", "SEX"))) %>%
  ggplot(aes(x = value, fill = name)) +
  geom_bar(alpha = 0.5, color = "black") +
  facet_wrap(~name, scales = "free") +
  scale_fill_manual(values = c("DOSE" = "lightgreen", "SEX" = "salmon"))
mavoplot6

#Saving the figure in the folder
mavoplot2_file <- here("fitting-exercise", "YxAgexDose.png")
ggsave(filename = mavoplot2_file, plot=mavoplot2, bg="white")


mavoplot3_file <- here("fitting-exercise", "YxHTxDose.png")
ggsave(filename = mavoplot3_file, plot=mavoplot3, bg="white")


mavoplot4_file <- here("fitting-exercise", "YxWTxDose.png")
ggsave(filename = mavoplot4_file, plot=mavoplot4, bg="white")

mavoplot5_file <- here("fitting-exercise", "factorplots.png")
ggsave(filename = mavoplot5_file, plot=mavoplot5, bg="white")


mavoplot6_file <- here("fitting-exercise", "YxDose.png")
ggsave(filename = mavoplot6_file, plot=mavoplot6, bg="white")
```


## Distributions of Variables

These histograms visualize the distributions of variables such as total
drug ('Y'), age, weight ('WT'), and height ('HT'). They help identify
any potential outliers or unusual patterns in the data.

```{r}
mavohist1 <-
  cleaned_mavo %>%
  select(Y, AGE, WT, HT) %>%
  pivot_longer(everything()) %>%
  mutate(name = factor(name, levels = c("Y", "AGE", "WT", "HT"))) %>%  #Keeps the order of plot
  ggplot(aes(x = value, fill = name)) +
  geom_histogram(alpha = 0.5, color="black") +
  facet_wrap(~name, scales = "free") +
  scale_fill_manual(values = c("Y" = "green", "AGE" = "blue", "WT" = "grey", "HT" = "salmon")) +
  theme_minimal()
mavohist1

#Saving the figure in the folder
mavohist1_file <- here("fitting-exercise", "histo_plots.png")
ggsave(filename = mavohist1_file, plot=mavohist1, bg="white")
```

## Pair/Correlation Plots

The pair plot provides a visual overview of the relationships between
the variables 'Y', age, weight, and height. The correlation matrix
quantifies the strength and direction of the linear relationships
between these variables. It helps identify potential multicollinearity
issues and informs feature selection for modeling purposes.

```{r}
# Using the pairs function for selected variables
mavo_matrix <- pairs(cleaned_mavo[, c("Y", "AGE", "WT", "HT")], 
      main = "Pairwise Scatterplot Matrix")

mavo_matrix

#Saving the figure in the folder
correl_file <- here("fitting-exercise", "pair_matrix.png")
ggsave(filename = correl_file, plot=mavo_matrix, bg="white")
```

# Model Fitting

Here, I conduct a model fitting to assess the relationship of the
outcome of interest with the other variables. Tidymodels provides the
functions necessary for modeling and preprocessing data. The recipe
function specifies the data preprocessing steps. In this case, the
outcome variable (Y) is defined as the response variable, and all other
variables are considered predictors. step_dummy converts categorical
predictors into dummy variables, step_center centers numeric predictors
around their mean, and step_scale scales numeric predictors to have unit
variance.

```{r}
# Load necessary libraries
library(tidymodels)
# Helper packages
library(readr)       # for importing data
library(broom.mixed) # for converting bayesian models to tidy tibbles
library(dotwhisker)  # for visualizing regression results
library(tidymodels)

# Define the recipe
mavo_recipe <- recipe(Y ~ ., data = cleaned_mavo) %>%
  step_dummy(all_nominal(), -all_outcomes()) %>%
  step_center(all_predictors()) %>%
  step_scale(all_predictors())
```

The initial_split function divides the dataset into training and testing
sets. The prop argument specifies the proportion of the data to allocate
to the training set (80% in this case), and the strata argument ensures
that the split is stratified based on the outcome variable (Y), which
helps maintain the balance of categories in both sets.

The linear_reg function specifies the linear regression model.
set_engine("lm") selects the linear model engine (ordinary least squares
regression), and set_mode("regression") sets the mode of the model to
regression, indicating that it predicts a continuous outcome (Y).

The workflow function creates a modeling workflow. This workflow
combines the preprocessing steps defined in the recipe with the
specified model. The add_recipe function adds the recipe to the
workflow, and the add_model function adds the linear regression model.

```{r}
# Split the data into training and testing sets
set.seed(123) # for reproducibility
mavo_split <- initial_split(cleaned_mavo, prop = 0.8, strata = Y)
mavo_train <- training(mavo_split)
mavo_test <- testing(mavo_split)

# Define the linear regression model specification
linear_spec <- linear_reg() %>%
  set_engine("lm") %>%
  set_mode("regression")

# Create a workflow that incorporates the recipe and the model
mavo_workflow <- workflow() %>%
  add_recipe(mavo_recipe) %>%
  add_model(linear_spec)
```

The fit function fits the workflow to the training data. This step
applies the preprocessing steps defined in the recipe to the training
data and then fits the linear regression model to the preprocessed data.

The predict function generates predictions from the fitted model using
the testing data. This step applies the preprocessing steps (from the
recipe) to the testing data and then predicts the outcome variable (Y)
using the fitted linear regression model.

The metrics function computes evaluation metrics for the model
predictions. Here, we calculate the root mean squared error (RMSE) and
the R-squared value to assess the model's performance. The computed
metrics are then extracted and stored in the variables all_rmse and
all_r_squared, respectively.

```{r}
# Fit the workflow to the training data
mavo_fit <- fit(mavo_workflow, data = mavo_train)

# Compute predictions on the testing data
mavo_predictions <- predict(mavo_fit, new_data = mavo_test) %>%
  bind_cols(mavo_test)
```

The metrics function computes evaluation metrics for the model
predictions. Here, we calculate the root mean squared error (RMSE) and
the R-squared value to assess the model's performance. The computed
metrics are then extracted and stored in the variables all_rmse and
all_r_squared, respectively.

```{r}
# Compute RMSE and R-squared for the model using all predictors
all_metrics <- mavo_predictions %>%
  metrics(truth = Y, estimate = .pred)

all_rmse <- all_metrics %>%
  filter(.metric == "rmse") %>%
  pull(.estimate)

all_r_squared <- all_metrics %>%
  filter(.metric == "rsq") %>%
  pull(.estimate)
```

Finally, we print the computed RMSE and R-squared values to evaluate the
model's performance. These metrics provide insights into how well the
linear regression model fits the data and predicts the outcome variable.

```{r}
# Print RMSE and R-squared for the model using all predictors
cat("Model using all predictors:\n")
cat("RMSE:", all_rmse, "\n")
cat("R-squared:", all_r_squared, "\n")

mavo_predictions
mavo_fit
```

## Results

The RMSE is approximately 654.5275. Lower values of RMSE indicate better
model performance, as they imply smaller errors between predicted and
actual values. The R-squared value is approximately 0.492, suggesting
that the model explains about 49.2% of the variance in the outcome
variable. 


# Modeling Fitting 2

The data is randomly splitted into 75% train and 25% test sets following the [Data Splitting sectoin of the get Started tidymodels tutorial](https://www.tidymodels.org/start/recipes/#data-split).Linear model is fitted on the training data. In later part of the exercise the model performance will be measured by applying it on the testing data.

```{r}
rngseed <- 1234
```

```{r}
# Remove the "RACE" variable from dataset
mavo2 <- subset(cleaned_mavo, select = c("Y", "DOSE", "AGE", "SEX", "WT", "HT"))
```

```{r}
#setting the random seed for reproducibility
set.seed(rngseed)

#Assig 75% of the data into the training set
mavodata_split <- initial_split(mavo2, prop = .75)

#Create data frames for the train and test data
mavo2train_data <- training(mavodata_split)
mavo2test_data <- testing(mavodata_split)

#Check data structure
str(mavo2train_data)
str(mavo2test_data)
```

## Linear Model

Two linear models are fitted to the continuous outcome (Y) of the train data. The first uses only the main predictor of interest DOSE, and the second uses all predictors. A null model is also fitted.

```{r}
#For reproducibility
set.seed(rngseed)

#Using linear regression function from tidymodels.
mavo2lin_mod <- linear_reg() %>% set_engine("lm") 

#Fit Y on Dose
mavlinfit_dose <- mavo2lin_mod%>%fit(Y ~ DOSE, data = mavo2train_data)

#Fit Y on All variables in the data
mavlinfit_all <- mavo2lin_mod%>%fit(Y~., data= mavo2train_data)

# Fitting a null model using tidymodels' parsnip engine
mavo2null_mod <- null_model() %>% set_engine("parsnip") %>% set_mode("regression")
mavlinfit_null <- mavo2null_mod %>%
  fit(Y ~ 1, data = mavo2train_data)

#Use tidy for clean formatting of table
tidy(mavlinfit_dose)
tidy(mavlinfit_all)
tidy(mavlinfit_null)
```

Both the first and second models indicate a positve relationship between Y and DOSE. Furthermore, the second model implies that even when considering the influence of other factors, Y shows a positive correlation with AGE and a negative correlation with both WT and HT. With all other factors held constant, a change in the SEX variable from 1 to 2 is expected to result in a decrease in Y.

## Root Mean Square Error
Subsequently, RMSE metrics are calculated for all models using the methods outlined by Dr. Handel for solving module 8 exercises.

```{r}
#| message: false
#| warning: false
#Computing the RMSE for model 1
mavmetrics_dose <- mavlinfit_dose %>%
  predict(mavo2train_data) %>%
  bind_cols(mavo2train_data)%>%
  metrics(truth=Y, estimate=.pred)

#Computing the RMSE for model 2
mavmetrics_all <- mavlinfit_all %>%
  predict(mavo2train_data) %>%
  bind_cols(mavo2train_data)%>%
  metrics(truth=Y, estimate=.pred)

#Computing the RMSE for model 3
mavmetrics_null <- mavlinfit_null %>%
  predict(mavo2train_data) %>%
  bind_cols(mavo2train_data)%>%
  metrics(truth=Y, estimate=.pred)


#print the results
print(mavmetrics_null)
print(mavmetrics_dose)
print(mavmetrics_all)
```

The RMSE values stand at 948, 702, and 627 for the null model, the model with only DOSE, and the model with all predictors, respectively. RMSE serves as a gauge for the average disparity between the model's predicted values and the actual data. A lower RMSE signifies a superior fit to the data. Consequently, based on the RMSE metrics, the linear model comprising all predictor variables demonstrates superior performance compared to the other two models.

## Cross-Validation
Following that, the model performance is evaluated using the cross-validation technique employing 10 folds for the two main models. This technique involves dividing the training data into 10 subsets and iteratively using 9 of them for model fitting while reserving the remaining 10% for evaluation. This process is repeated 10 times, ensuring that each subset serves as both training and evaluation data. 
```{r}
#setting the random seed for reproducibility
set.seed(rngseed)

folds <-vfold_cv(mavo2train_data, v=10)
folds
```

Subsequently, an object for resampling is constructed using the 'workflow' function from tidymodels. This function combines pre-processing, modeling, and post-processing instructions into a single entity, streamlining the analysis pipeline.

```{r}
#setting the random seed for reproducibility
set.seed(rngseed)

#Resampling using workflow for the model with only DOSE as predictor
mavlinfit_dose2 <- 
	workflow() %>%
	add_model(mavo2lin_mod) %>%
  add_formula(Y ~ DOSE)%>%
	fit_resamples(folds)

#Resampling using workflow for for the model with all predictors
mavlinfit_all2 <- 
	workflow() %>%
	add_model(mavo2lin_mod) %>%
  add_formula(Y ~ .)%>%
	fit_resamples(folds)

#extracting the performance statistics results created from the 10 assessment sets. 

collect_metrics(mavlinfit_dose2)
collect_metrics(mavlinfit_all2)
```

The implementation of 10-fold cross-validation resulted in a notable alteration in the RMSE values of the models. Specifically, for the model containing only the DOSE predictor, the RMSE decreased to 690 from the original 702 observed during training data utilization without cross-validation. Conversely, the model incorporating all predictors saw its RMSE increase to 645 with 10-fold cross-validation, compared to the initial value of 627 without it.

In contrast to the train/test model without cross-validation, the 10-fold cross-validation computes 10 distinct RMSE values for each sample and then averages these values. This process introduces variability among the resulting RMSEs. Further analysis indicated that the full model displays a smaller standard error of 64.81 for RMSE, in contrast to the 67.49 observed for the model including only the DOSE predictor.

### CV-repeat
Lastly, the 10-fold cross-validation modeling is repeated to assess the variation in the metric when employing a different randomization seed.

```{r}
#setting a different random seed
set.seed(222)

#Assigning 75% of the data into the training set
mavodata_split2 <- initial_split(mavo2, prop = .75)

#Creating data frames for the train and test data
mavotrain_data2 <- training(mavodata_split2)
mavotest_data2 <- testing(mavodata_split2)
```

Preparing the data for 10-fold cross-validation.
```{r}
#setting a different random seed
set.seed(222)

#Creating 10 random samples of the newly generated training data
folds2 <-vfold_cv(mavotrain_data2, v=10)
folds2
```

Using the workflow to compute both the models.
```{r}
#setting the random seed for reproducibility
set.seed(222)

#Resampling using workflow for the model with only DOSE as predictor
mavolinfit_dose3 <- workflow() %>%
	add_model(mavo2lin_mod) %>%
  add_formula(Y ~ DOSE)%>%
	fit_resamples(folds2)

#Resampling using workflowfor for the model with all predictors
mavolinfit_all3 <- workflow() %>%
	add_model(mavo2lin_mod) %>%
  add_formula(Y ~ .)%>%
	fit_resamples(folds2)

#extracting the performance statistics results created from the 10 assessment sets. 

collect_metrics(mavolinfit_dose3)
collect_metrics(mavolinfit_all3)
```

