[
  {
    "objectID": "fitting-exercise/fitting-exercise.html",
    "href": "fitting-exercise/fitting-exercise.html",
    "title": "Fitting Exercise",
    "section": "",
    "text": "For this assignment, I will be using data on a drug candidate called Mavoglurant from the paper:\nWendling, T., Dumitras, S., Ogungbenro, K. et al. Application of a Bayesian approach to physiological modelling of mavoglurant population pharmacokinetics. J Pharmacokinet Pharmacodyn 42, 639–657 (2015). https://doi.org/10.1007/s10928-015-9430-4\n\n\nFirst we will load the data and additional packages necessary for the assigniment.\n\nlibrary(readr) #for loading Excel files\nlibrary(dplyr) #for data processing/cleaning\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\nlibrary(tidyr) #for data processing/cleaning\nlibrary(skimr) #for nice visualization of data \nlibrary(here) #to set paths\n\nhere() starts at /Users/mutsa_n/Desktop/MADA-course/mutsanyamuranga-MADA-portfolio\n\nlibrary(ggplot2) # for plots\nlibrary(gtsummary)# for summary tables\n\n#BlackLivesMatter\n\nlibrary(patchwork) #for combine plots\nlibrary(tidymodels)\n\n── Attaching packages ────────────────────────────────────── tidymodels 1.1.1 ──\n\n\n✔ broom        1.0.5      ✔ rsample      1.2.0 \n✔ dials        1.2.1      ✔ tibble       3.2.1 \n✔ infer        1.0.6      ✔ tune         1.1.2 \n✔ modeldata    1.3.0      ✔ workflows    1.1.4 \n✔ parsnip      1.2.0      ✔ workflowsets 1.0.1 \n✔ purrr        1.0.2      ✔ yardstick    1.2.0 \n✔ recipes      1.0.10     \n\n\n── Conflicts ───────────────────────────────────────── tidymodels_conflicts() ──\n✖ recipes::all_double()  masks gtsummary::all_double()\n✖ recipes::all_factor()  masks gtsummary::all_factor()\n✖ recipes::all_integer() masks gtsummary::all_integer()\n✖ recipes::all_logical() masks gtsummary::all_logical()\n✖ recipes::all_numeric() masks gtsummary::all_numeric()\n✖ purrr::discard()       masks scales::discard()\n✖ dplyr::filter()        masks stats::filter()\n✖ dplyr::lag()           masks stats::lag()\n✖ yardstick::spec()      masks readr::spec()\n✖ recipes::step()        masks stats::step()\n• Dig deeper into tidy modeling with R at https://www.tmwr.org\n\n\nI will also load the data from the another paper which has used this data to ensure a level of consistency and ease of usage with the data.\n\n# Loading Data through CSV\nmavodrug &lt;-- read.csv('Mavoglurant_A2121_nmpk.csv')\nmavodrug &lt;- abs(mavodrug)"
  },
  {
    "objectID": "fitting-exercise/fitting-exercise.html#loading",
    "href": "fitting-exercise/fitting-exercise.html#loading",
    "title": "Fitting Exercise",
    "section": "",
    "text": "First we will load the data and additional packages necessary for the assigniment.\n\nlibrary(readr) #for loading Excel files\nlibrary(dplyr) #for data processing/cleaning\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\nlibrary(tidyr) #for data processing/cleaning\nlibrary(skimr) #for nice visualization of data \nlibrary(here) #to set paths\n\nhere() starts at /Users/mutsa_n/Desktop/MADA-course/mutsanyamuranga-MADA-portfolio\n\nlibrary(ggplot2) # for plots\nlibrary(gtsummary)# for summary tables\n\n#BlackLivesMatter\n\nlibrary(patchwork) #for combine plots\nlibrary(tidymodels)\n\n── Attaching packages ────────────────────────────────────── tidymodels 1.1.1 ──\n\n\n✔ broom        1.0.5      ✔ rsample      1.2.0 \n✔ dials        1.2.1      ✔ tibble       3.2.1 \n✔ infer        1.0.6      ✔ tune         1.1.2 \n✔ modeldata    1.3.0      ✔ workflows    1.1.4 \n✔ parsnip      1.2.0      ✔ workflowsets 1.0.1 \n✔ purrr        1.0.2      ✔ yardstick    1.2.0 \n✔ recipes      1.0.10     \n\n\n── Conflicts ───────────────────────────────────────── tidymodels_conflicts() ──\n✖ recipes::all_double()  masks gtsummary::all_double()\n✖ recipes::all_factor()  masks gtsummary::all_factor()\n✖ recipes::all_integer() masks gtsummary::all_integer()\n✖ recipes::all_logical() masks gtsummary::all_logical()\n✖ recipes::all_numeric() masks gtsummary::all_numeric()\n✖ purrr::discard()       masks scales::discard()\n✖ dplyr::filter()        masks stats::filter()\n✖ dplyr::lag()           masks stats::lag()\n✖ yardstick::spec()      masks readr::spec()\n✖ recipes::step()        masks stats::step()\n• Dig deeper into tidy modeling with R at https://www.tmwr.org\n\n\nI will also load the data from the another paper which has used this data to ensure a level of consistency and ease of usage with the data.\n\n# Loading Data through CSV\nmavodrug &lt;-- read.csv('Mavoglurant_A2121_nmpk.csv')\nmavodrug &lt;- abs(mavodrug)"
  },
  {
    "objectID": "fitting-exercise/fitting-exercise.html#summary-tables",
    "href": "fitting-exercise/fitting-exercise.html#summary-tables",
    "title": "Fitting Exercise",
    "section": "Summary Tables",
    "text": "Summary Tables\nThe summary tables provide a quick overview of the numerical and categorical variables in the dataset.\n\nmavotable1 &lt;- tbl_summary(\n  cleaned_mavo,\n  by = DOSE, # Stratify summary by DOSE\n  type = list(\n    DOSE ~ \"categorical\", #Specifying DOSE as categorical\n    SEX ~ \"categorical\",  #Specifying SES as categorical\n    Y ~ \"continuous2\",\n    AGE ~ \"continuous2\",\n    WT ~ \"continuous2\",\n    HT ~ \"continuous2\"\n  ),\n  statistic = list(\n    all_continuous() ~ c(\"{mean} ({sd})\", \"{min}, {max}\"), # Statistics for continuous variables\n    all_categorical() ~ \"{n} ({p}%)\"), # Statistics for categorical variables\n  missing = \"no\" # Option to exclude missing data in summary\n)\n\nmavotable1\n\n\n\n\n\n  \n    \n      Characteristic\n      25, N = 591\n      37.5, N = 121\n      50, N = 491\n    \n  \n  \n    Y\n\n\n\n        Mean (SD)\n1,783 (601)\n2,464 (488)\n3,239 (787)\n        Range\n826, 3,866\n1,801, 3,463\n1,949, 5,607\n    AGE\n\n\n\n        Mean (SD)\n32 (9)\n36 (10)\n33 (9)\n        Range\n18, 49\n19, 50\n18, 49\n    SEX\n\n\n\n        1\n49 (83%)\n10 (83%)\n45 (92%)\n        2\n10 (17%)\n2 (17%)\n4 (8.2%)\n    WT\n\n\n\n        Mean (SD)\n81 (12)\n81 (11)\n84 (13)\n        Range\n58, 111\n64, 102\n57, 115\n    HT\n\n\n\n        Mean (SD)\n1.76 (0.09)\n1.75 (0.10)\n1.76 (0.08)\n        Range\n1.56, 1.91\n1.56, 1.91\n1.52, 1.93\n  \n  \n  \n    \n      1 n (%)"
  },
  {
    "objectID": "fitting-exercise/fitting-exercise.html#plots",
    "href": "fitting-exercise/fitting-exercise.html#plots",
    "title": "Fitting Exercise",
    "section": "Plots",
    "text": "Plots\nThese plots show the relationship between the total drug (‘Y’) and other predictors such as age, dose, and sex. Scatterplots help visualize the continuous relationship between ‘Y’ and age, while boxplots illustrate the distribution of ‘Y’ across different levels of dose and sex.\n\nmavoplot2 &lt;- cleaned_mavo %&gt;%\n  mutate(DOSE = as.factor(DOSE)) %&gt;% # Convert DOSE to factor here\n  ggplot(aes(x = AGE, y = Y, group = DOSE, col = DOSE)) +\n  geom_point() + \n  geom_smooth(method = lm, se = FALSE) + # Add a linear regression line\n  scale_color_viridis_d(option = \"plasma\", end = .7)+\n  labs(title = \"Scatterplot of Y vs. AGE\", x = \"Age\", y = \"Total Drug (Y)\") +\n  theme_minimal()\nmavoplot2\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\nmavoplot3 &lt;- cleaned_mavo %&gt;%\n  mutate(DOSE = as.factor(DOSE)) %&gt;% # Convert DOSE to factor here\n  ggplot(aes(x = HT, y = Y, group = DOSE, col = DOSE)) +\n  geom_point() + \n  geom_smooth(method = lm, se = FALSE) + # Add a linear regression line\n  scale_color_viridis_d(option = \"inferno\", end = .7)+\n  labs(title = \"Scatterplot of Y vs. HT\", x = \"HT\", y = \"Total Drug (Y)\") +\n  theme_minimal()\nmavoplot3\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\nmavoplot4&lt;-cleaned_mavo %&gt;%\n  mutate(DOSE = as.factor(DOSE)) %&gt;% # Convert DOSE to factor here\n  ggplot(aes(x = WT, y = Y, group = DOSE, col = DOSE)) +\n  geom_point() + \n  geom_smooth(method = lm, se = FALSE) + # Add a linear regression line\n  scale_color_viridis_d(option = \"magma\", end = .7)+\n  labs(title = \"Scatterplot of Y vs. WT\", x = \"WT\", y = \"Total Drug (Y)\") +\n  theme_minimal()\nmavoplot4\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\nmavoplot5 &lt;- cleaned_mavo %&gt;%\n  mutate(DOSE = as.factor(DOSE)) %&gt;% # Converting DOSE to factor only for the plot\n  ggplot(aes(x = DOSE, y = Y, fill = DOSE)) + \n  geom_boxplot() + \n  labs(title = \"Distribution of Y by DOSE\", x = \"DOSE\", y = \"Y\") +\n  theme_minimal()\nmavoplot5\n\n\n\nmavoplot6 &lt;- cleaned_mavo %&gt;%\n  mutate(DOSE = as.factor(DOSE)) %&gt;% # Convert DOSE to factor here\n  select(DOSE, SEX) %&gt;%\n  pivot_longer(everything(), names_to = \"name\", values_to = \"value\") %&gt;%\n  mutate(name = factor(name, levels = c(\"DOSE\", \"SEX\"))) %&gt;%\n  ggplot(aes(x = value, fill = name)) +\n  geom_bar(alpha = 0.5, color = \"black\") +\n  facet_wrap(~name, scales = \"free\") +\n  scale_fill_manual(values = c(\"DOSE\" = \"lightgreen\", \"SEX\" = \"salmon\"))\nmavoplot6\n\n\n\n#Saving the figure in the folder\nmavoplot2_file &lt;- here(\"fitting-exercise\", \"YxAgexDose.png\")\nggsave(filename = mavoplot2_file, plot=mavoplot2, bg=\"white\")\n\nSaving 7 x 5 in image\n`geom_smooth()` using formula = 'y ~ x'\n\nmavoplot3_file &lt;- here(\"fitting-exercise\", \"YxHTxDose.png\")\nggsave(filename = mavoplot3_file, plot=mavoplot3, bg=\"white\")\n\nSaving 7 x 5 in image\n`geom_smooth()` using formula = 'y ~ x'\n\nmavoplot4_file &lt;- here(\"fitting-exercise\", \"YxWTxDose.png\")\nggsave(filename = mavoplot4_file, plot=mavoplot4, bg=\"white\")\n\nSaving 7 x 5 in image\n`geom_smooth()` using formula = 'y ~ x'\n\nmavoplot5_file &lt;- here(\"fitting-exercise\", \"factorplots.png\")\nggsave(filename = mavoplot5_file, plot=mavoplot5, bg=\"white\")\n\nSaving 7 x 5 in image\n\nmavoplot6_file &lt;- here(\"fitting-exercise\", \"YxDose.png\")\nggsave(filename = mavoplot6_file, plot=mavoplot6, bg=\"white\")\n\nSaving 7 x 5 in image"
  },
  {
    "objectID": "fitting-exercise/fitting-exercise.html#distributions-of-variables",
    "href": "fitting-exercise/fitting-exercise.html#distributions-of-variables",
    "title": "Fitting Exercise",
    "section": "Distributions of Variables",
    "text": "Distributions of Variables\nThese histograms visualize the distributions of variables such as total drug (‘Y’), age, weight (‘WT’), and height (‘HT’). They help identify any potential outliers or unusual patterns in the data.\n\nmavohist1 &lt;-\n  cleaned_mavo %&gt;%\n  select(Y, AGE, WT, HT) %&gt;%\n  pivot_longer(everything()) %&gt;%\n  mutate(name = factor(name, levels = c(\"Y\", \"AGE\", \"WT\", \"HT\"))) %&gt;%  #Keeps the order of plot\n  ggplot(aes(x = value, fill = name)) +\n  geom_histogram(alpha = 0.5, color=\"black\") +\n  facet_wrap(~name, scales = \"free\") +\n  scale_fill_manual(values = c(\"Y\" = \"green\", \"AGE\" = \"blue\", \"WT\" = \"grey\", \"HT\" = \"salmon\")) +\n  theme_minimal()\nmavohist1\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n#Saving the figure in the folder\nmavohist1_file &lt;- here(\"fitting-exercise\", \"histo_plots.png\")\nggsave(filename = mavohist1_file, plot=mavohist1, bg=\"white\")\n\nSaving 7 x 5 in image\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`."
  },
  {
    "objectID": "fitting-exercise/fitting-exercise.html#paircorrelation-plots",
    "href": "fitting-exercise/fitting-exercise.html#paircorrelation-plots",
    "title": "Fitting Exercise",
    "section": "Pair/Correlation Plots",
    "text": "Pair/Correlation Plots\nThe pair plot provides a visual overview of the relationships between the variables ‘Y’, age, weight, and height. The correlation matrix quantifies the strength and direction of the linear relationships between these variables. It helps identify potential multicollinearity issues and informs feature selection for modeling purposes.\n\n# Using the pairs function for selected variables\nmavo_matrix &lt;- pairs(cleaned_mavo[, c(\"Y\", \"AGE\", \"WT\", \"HT\")], \n      main = \"Pairwise Scatterplot Matrix\")\n\n\n\nmavo_matrix\n\nNULL\n\n#Saving the figure in the folder\ncorrel_file &lt;- here(\"fitting-exercise\", \"pair_matrix.png\")\nggsave(filename = correl_file, plot=mavo_matrix, bg=\"white\")\n\nSaving 7 x 5 in image"
  },
  {
    "objectID": "fitting-exercise/fitting-exercise.html#results",
    "href": "fitting-exercise/fitting-exercise.html#results",
    "title": "Fitting Exercise",
    "section": "Results",
    "text": "Results\nThe RMSE is approximately 654.5275. Lower values of RMSE indicate better model performance, as they imply smaller errors between predicted and actual values. The R-squared value is approximately 0.492, suggesting that the model explains about 49.2% of the variance in the outcome variable."
  },
  {
    "objectID": "fitting-exercise/fitting-exercise.html#linear-model",
    "href": "fitting-exercise/fitting-exercise.html#linear-model",
    "title": "Fitting Exercise",
    "section": "Linear Model",
    "text": "Linear Model\nTwo linear models are fitted to the continuous outcome (Y) of the train data. The first uses only the main predictor of interest DOSE, and the second uses all predictors. A null model is also fitted.\n\n#For reproducibility\nset.seed(rngseed)\n\n#Using linear regression function from tidymodels.\nmavo2lin_mod &lt;- linear_reg() %&gt;% set_engine(\"lm\") \n\n#Fit Y on Dose\nmavlinfit_dose &lt;- mavo2lin_mod%&gt;%fit(Y ~ DOSE, data = mavo2train_data)\n\n#Fit Y on All variables in the data\nmavlinfit_all &lt;- mavo2lin_mod%&gt;%fit(Y~., data= mavo2train_data)\n\n# Fitting a null model using tidymodels' parsnip engine\nmavo2null_mod &lt;- null_model() %&gt;% set_engine(\"parsnip\") %&gt;% set_mode(\"regression\")\nmavlinfit_null &lt;- mavo2null_mod %&gt;%\n  fit(Y ~ 1, data = mavo2train_data)\n\n#Use tidy for clean formatting of table\ntidy(mavlinfit_dose)\n\n# A tibble: 2 × 5\n  term        estimate std.error statistic  p.value\n  &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;\n1 (Intercept)    535.     244.        2.19 3.08e- 2\n2 DOSE            53.4      6.29      8.50 4.41e-13\n\ntidy(mavlinfit_all)\n\n# A tibble: 6 × 5\n  term         estimate std.error statistic  p.value\n  &lt;chr&gt;           &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;\n1 (Intercept)  4397.      2170.      2.03   4.59e- 2\n2 DOSE           55.3        5.83    9.49   6.09e-15\n3 AGE            -0.417      9.50   -0.0439 9.65e- 1\n4 SEX2         -569.       285.     -1.99   4.95e- 2\n5 WT            -22.6        7.65   -2.96   4.00e- 3\n6 HT          -1130.      1358.     -0.832  4.08e- 1\n\ntidy(mavlinfit_null)\n\n# A tibble: 1 × 1\n  value\n  &lt;dbl&gt;\n1 2509.\n\n\nBoth the first and second models indicate a positve relationship between Y and DOSE. Furthermore, the second model implies that even when considering the influence of other factors, Y shows a positive correlation with AGE and a negative correlation with both WT and HT. With all other factors held constant, a change in the SEX variable from 1 to 2 is expected to result in a decrease in Y."
  },
  {
    "objectID": "fitting-exercise/fitting-exercise.html#root-mean-square-error",
    "href": "fitting-exercise/fitting-exercise.html#root-mean-square-error",
    "title": "Fitting Exercise",
    "section": "Root Mean Square Error",
    "text": "Root Mean Square Error\nSubsequently, RMSE metrics are calculated for all models using the methods outlined by Dr. Handel for solving module 8 exercises.\n\n#Computing the RMSE for model 1\nmavmetrics_dose &lt;- mavlinfit_dose %&gt;%\n  predict(mavo2train_data) %&gt;%\n  bind_cols(mavo2train_data)%&gt;%\n  metrics(truth=Y, estimate=.pred)\n\n#Computing the RMSE for model 2\nmavmetrics_all &lt;- mavlinfit_all %&gt;%\n  predict(mavo2train_data) %&gt;%\n  bind_cols(mavo2train_data)%&gt;%\n  metrics(truth=Y, estimate=.pred)\n\n#Computing the RMSE for model 3\nmavmetrics_null &lt;- mavlinfit_null %&gt;%\n  predict(mavo2train_data) %&gt;%\n  bind_cols(mavo2train_data)%&gt;%\n  metrics(truth=Y, estimate=.pred)\n\n\n#print the results\nprint(mavmetrics_null)\n\n# A tibble: 3 × 3\n  .metric .estimator .estimate\n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;\n1 rmse    standard        948.\n2 rsq     standard         NA \n3 mae     standard        765.\n\nprint(mavmetrics_dose)\n\n# A tibble: 3 × 3\n  .metric .estimator .estimate\n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;\n1 rmse    standard     703.   \n2 rsq     standard       0.451\n3 mae     standard     546.   \n\nprint(mavmetrics_all)\n\n# A tibble: 3 × 3\n  .metric .estimator .estimate\n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;\n1 rmse    standard     627.   \n2 rsq     standard       0.562\n3 mae     standard     486.   \n\n\nThe RMSE values stand at 948, 702, and 627 for the null model, the model with only DOSE, and the model with all predictors, respectively. RMSE serves as a gauge for the average disparity between the model’s predicted values and the actual data. A lower RMSE signifies a superior fit to the data. Consequently, based on the RMSE metrics, the linear model comprising all predictor variables demonstrates superior performance compared to the other two models."
  },
  {
    "objectID": "fitting-exercise/fitting-exercise.html#cross-validation",
    "href": "fitting-exercise/fitting-exercise.html#cross-validation",
    "title": "Fitting Exercise",
    "section": "Cross-Validation",
    "text": "Cross-Validation\nFollowing that, the model performance is evaluated using the cross-validation technique employing 10 folds for the two main models. This technique involves dividing the training data into 10 subsets and iteratively using 9 of them for model fitting while reserving the remaining 10% for evaluation. This process is repeated 10 times, ensuring that each subset serves as both training and evaluation data.\n\n#setting the random seed for reproducibility\nset.seed(rngseed)\n\nfolds &lt;-vfold_cv(mavo2train_data, v=10)\nfolds\n\n#  10-fold cross-validation \n# A tibble: 10 × 2\n   splits         id    \n   &lt;list&gt;         &lt;chr&gt; \n 1 &lt;split [81/9]&gt; Fold01\n 2 &lt;split [81/9]&gt; Fold02\n 3 &lt;split [81/9]&gt; Fold03\n 4 &lt;split [81/9]&gt; Fold04\n 5 &lt;split [81/9]&gt; Fold05\n 6 &lt;split [81/9]&gt; Fold06\n 7 &lt;split [81/9]&gt; Fold07\n 8 &lt;split [81/9]&gt; Fold08\n 9 &lt;split [81/9]&gt; Fold09\n10 &lt;split [81/9]&gt; Fold10\n\n\nSubsequently, an object for resampling is constructed using the ‘workflow’ function from tidymodels. This function combines pre-processing, modeling, and post-processing instructions into a single entity, streamlining the analysis pipeline.\n\n#setting the random seed for reproducibility\nset.seed(rngseed)\n\n#Resampling using workflow for the model with only DOSE as predictor\nmavlinfit_dose2 &lt;- \n    workflow() %&gt;%\n    add_model(mavo2lin_mod) %&gt;%\n  add_formula(Y ~ DOSE)%&gt;%\n    fit_resamples(folds)\n\n#Resampling using workflow for for the model with all predictors\nmavlinfit_all2 &lt;- \n    workflow() %&gt;%\n    add_model(mavo2lin_mod) %&gt;%\n  add_formula(Y ~ .)%&gt;%\n    fit_resamples(folds)\n\n#extracting the performance statistics results created from the 10 assessment sets. \n\ncollect_metrics(mavlinfit_dose2)\n\n# A tibble: 2 × 6\n  .metric .estimator    mean     n std_err .config             \n  &lt;chr&gt;   &lt;chr&gt;        &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;               \n1 rmse    standard   691.       10 67.5    Preprocessor1_Model1\n2 rsq     standard     0.512    10  0.0592 Preprocessor1_Model1\n\ncollect_metrics(mavlinfit_all2)\n\n# A tibble: 2 × 6\n  .metric .estimator    mean     n std_err .config             \n  &lt;chr&gt;   &lt;chr&gt;        &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;               \n1 rmse    standard   646.       10 64.8    Preprocessor1_Model1\n2 rsq     standard     0.573    10  0.0686 Preprocessor1_Model1\n\n\nThe implementation of 10-fold cross-validation resulted in a notable alteration in the RMSE values of the models. Specifically, for the model containing only the DOSE predictor, the RMSE decreased to 690 from the original 702 observed during training data utilization without cross-validation. Conversely, the model incorporating all predictors saw its RMSE increase to 645 with 10-fold cross-validation, compared to the initial value of 627 without it.\nIn contrast to the train/test model without cross-validation, the 10-fold cross-validation computes 10 distinct RMSE values for each sample and then averages these values. This process introduces variability among the resulting RMSEs. Further analysis indicated that the full model displays a smaller standard error of 64.81 for RMSE, in contrast to the 67.49 observed for the model including only the DOSE predictor.\n\nCV-repeat\nLastly, the 10-fold cross-validation modeling is repeated to assess the variation in the metric when employing a different randomization seed.\n\n#setting a different random seed\nset.seed(222)\n\n#Assigning 75% of the data into the training set\nmavodata_split2 &lt;- initial_split(mavo2, prop = .75)\n\n#Creating data frames for the train and test data\nmavotrain_data2 &lt;- training(mavodata_split2)\nmavotest_data2 &lt;- testing(mavodata_split2)\n\nPreparing the data for 10-fold cross-validation.\n\n#setting a different random seed\nset.seed(222)\n\n#Creating 10 random samples of the newly generated training data\nfolds2 &lt;-vfold_cv(mavotrain_data2, v=10)\nfolds2\n\n#  10-fold cross-validation \n# A tibble: 10 × 2\n   splits         id    \n   &lt;list&gt;         &lt;chr&gt; \n 1 &lt;split [81/9]&gt; Fold01\n 2 &lt;split [81/9]&gt; Fold02\n 3 &lt;split [81/9]&gt; Fold03\n 4 &lt;split [81/9]&gt; Fold04\n 5 &lt;split [81/9]&gt; Fold05\n 6 &lt;split [81/9]&gt; Fold06\n 7 &lt;split [81/9]&gt; Fold07\n 8 &lt;split [81/9]&gt; Fold08\n 9 &lt;split [81/9]&gt; Fold09\n10 &lt;split [81/9]&gt; Fold10\n\n\nUsing the workflow to compute both the models.\n\n#setting the random seed for reproducibility\nset.seed(222)\n\n#Resampling using workflow for the model with only DOSE as predictor\nmavolinfit_dose3 &lt;- workflow() %&gt;%\n    add_model(mavo2lin_mod) %&gt;%\n  add_formula(Y ~ DOSE)%&gt;%\n    fit_resamples(folds2)\n\n#Resampling using workflowfor for the model with all predictors\nmavolinfit_all3 &lt;- workflow() %&gt;%\n    add_model(mavo2lin_mod) %&gt;%\n  add_formula(Y ~ .)%&gt;%\n    fit_resamples(folds2)\n\n#extracting the performance statistics results created from the 10 assessment sets. \n\ncollect_metrics(mavolinfit_dose3)\n\n# A tibble: 2 × 6\n  .metric .estimator    mean     n std_err .config             \n  &lt;chr&gt;   &lt;chr&gt;        &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;               \n1 rmse    standard   665.       10 67.9    Preprocessor1_Model1\n2 rsq     standard     0.553    10  0.0608 Preprocessor1_Model1\n\ncollect_metrics(mavolinfit_all3)\n\n# A tibble: 2 × 6\n  .metric .estimator    mean     n std_err .config             \n  &lt;chr&gt;   &lt;chr&gt;        &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;               \n1 rmse    standard   635.       10 65.4    Preprocessor1_Model1\n2 rsq     standard     0.600    10  0.0691 Preprocessor1_Model1"
  },
  {
    "objectID": "fitting-exercise/fitting-exercise.html#model-prediction",
    "href": "fitting-exercise/fitting-exercise.html#model-prediction",
    "title": "Fitting Exercise",
    "section": "Model Prediction",
    "text": "Model Prediction\nI first create three data frames that combines the observed and predicted values from the 3 original model fits to all of the training data.\n\n# Creating a data-frame with observed and predicted values from the model with `DOSE` as the predictor\ndose_fit &lt;- mavlinfit_dose %&gt;%\n  predict(mavo2train_data) %&gt;%\n  bind_cols(mavo2train_data)\n\n# Creating a data-frame with observed and predicted values from the model with everything as the predictor\nall_fit &lt;- mavlinfit_all %&gt;%\n  predict(mavo2train_data) %&gt;%\n  bind_cols(mavo2train_data)\n\n# Creating a data-frame with observed and predicted values from the model\nnull_fit &lt;- mavlinfit_null %&gt;%\n  predict(mavo2train_data) %&gt;%\n  bind_cols(mavo2train_data)\n\nUsing ChatGPT, the following few lines were created to add a labeled column to each dataframe. Each data frames now have 90 observations and 8 variables each. After that, I combine all of the data frames with rbind() function. This leads to 270 observations and 8 variables, they are easily differentiable by the newly created label column.\n\n# For the dose_fit data frame, I added a label column and dubbed it 'Dose model'\ndose_fit$label &lt;- rep(\"Dose Model\")\n\n# For all_fit data frame, , I added a label column and dubbed it 'All model'\nall_fit$label &lt;- rep(\"All Model\")\n\n# For null_fit data frame, , I added a label column and dubbed it 'Null model'\nnull_fit$label &lt;- rep(\"Null Model\")\n\n# I combined the three different data frames, so I can create a graph with it with greater ease. \ncombined_fit &lt;- rbind(dose_fit, all_fit, null_fit)\n\nI then use ggplot to create plot to reflect how the model predictions performs visually. The plots differentiate each of the models by the label with color and shape for the data points. The graph plots the observed value vs the predicted value. A good model would have fairly similar predicted values to its observed value. Visually, that would show a more 1:1 or linear relationship. When observing the graph we created in p1 and p2, it becomes obvious that the model with all of the predictors show the greatest predictions. The Dose model and hte Null model both shows flat lines, which is easily explained by the properties of a null model and the distinct factor level characteristics of the DOSE predictor.\n\n# Create the ggplot figure to graph the predictive values vs the observed value for the three models\np1 &lt;- ggplot(\n  combined_fit, aes(x = Y, y = .pred, color = label, shape = label)) +\n  geom_point(size=2) +\n   scale_color_manual(values = c(\"#9467bd\", \"#ff9896\", \"#17becf\"))+\n  geom_abline(intercept = 0, slope = 1, linetype = \"dashed\") +  # Adding a 45-degree line\n  labs(x = \"Observed Values\", y = \"Predicted Values\", color = \"Model\", shape = \"Model\") +\n  xlim(0, 5000) + \n  ylim(0, 5000)+\n  theme_bw()\n\n# Viewing the plot\np1\n\nWarning: Removed 3 rows containing missing values (`geom_point()`).\n\n\n\n\n\nThe All Model does most clearly follow the 45 degree angle line compare to the other two models. Despite that, the All Model still shows some variation in the data points, which can be explained by other factors.\n\n# Create the same ggplot figure with facets\np2 &lt;- ggplot(\n  combined_fit, aes(x = Y, y = .pred, color = label)) +\n  geom_point() +\n  scale_color_manual(values = c(\"#9467bd\", \"#ff9896\", \"#17becf\"))+\n  geom_abline(intercept = 0, slope = 1, linetype = \"dashed\") +  # Adding a 45-degree line\n  labs(x = \"Observed Values\", y = \"Predicted Values\", color = \"Model\", shape = \"Model\") +\n  theme_minimal() +\n  facet_wrap(~ label, scales = \"free\")+ # Faceting by label (model)\n  xlim(0, 5000) + ylim(0, 5000)\n# Viewing the plot\np2\n\nWarning: Removed 3 rows containing missing values (`geom_point()`).\n\n\n\n\n\nI start with creating a column for residuals in the all_fit dataframe. I then plot the residuals versus the predicted to observe the patterns. The plot clearly shows a greater number of negative values compared to positive values, showing that there may be other factos still playing a role in affecting the data.\n\nall_fit &lt;- all_fit %&gt;%\n  mutate(residuals = .pred - Y)\n\np3 &lt;- ggplot(all_fit, aes(x = .pred, y = residuals)) +\n  geom_point(size = 2, color = \"#ff9896\") +\n  geom_hline(yintercept = 0, linetype = \"dashed\", color = \"black\") +  # Adding a line at y = 0\n  labs(x = \"Predicted Values\", y = \"Residuals\") +  # Corrected axis labels\n  ylim(-2500, 2500) +\n  theme_bw()\n\np3"
  },
  {
    "objectID": "fitting-exercise/fitting-exercise.html#model-prediction-and-uncertainty",
    "href": "fitting-exercise/fitting-exercise.html#model-prediction-and-uncertainty",
    "title": "Fitting Exercise",
    "section": "Model Prediction and Uncertainty",
    "text": "Model Prediction and Uncertainty\nThe following would be an attempt at calculating for uncertainty. This will be attempted by using 100 bootstrap samples. The following lines sets up a seed and creates 100 bootstraps into a object called bootstrap_100.\n\n# Set Seed for reproducibility\nset.seed(rngseed)\n\n# Creating 100 bootstraps using the training data\nbootstrap_100 &lt;- bootstraps(mavo2train_data, times=100)\n\nI created a function called fit_and_predict that allowed me to loop through each bootstrap and apply a linear model to each of them. A list of prediction is then generated for each of the bootstrap based on the training data. The predictions are then all compiled into a list. I applied that function to my object bootstrap_100 and set the output of that to prediction_list. I then converted that list to a matrix, which allowed me to run the code provided by Dr.Handel on the MADA website. That code calculated the confidence levels and median of the prediction variable.\n\nfit_and_predict &lt;- function(bootstrap_100) {\n  # Create an empty list to store predictions for all of the boots\n  prediction_list &lt;- list()\n  \n  # Loop through each bootstrap sample\n  for (i in 1:length(bootstrap_100$splits)) {\n    # Placing the current [i] bootstrap sample to 'bootstrap_sample object\n    bootstrap_sample &lt;- analysis(bootstrap_100$splits[[i]])\n    \n    # Fit a linear model\n    linear_model &lt;- lm(Y ~ ., data = bootstrap_sample)\n    \n    # Make predictions on the original train_data\n    boot_prediction &lt;- predict(linear_model, newdata = mavo2train_data)\n    \n    # Store predictions in the empty list created above\n    prediction_list[[i]] &lt;- boot_prediction\n  }\n  \n  return(prediction_list)\n}\n\n\n# Apply fit_and_predict function directly on bootstrap samples. This will cycle the function through each bootstrap sample\npredictions_list &lt;- fit_and_predict(bootstrap_100)\n\n# Verifying it worked. Running this should give me the 90 predicted observation for the first bootstrap sample\npredictions_list[1]\n\n[[1]]\n       1        2        3        4        5        6        7        8 \n3118.049 1894.039 2628.676 2165.134 2710.403 1370.259 2358.081 1990.458 \n       9       10       11       12       13       14       15       16 \n1601.518 2484.859 1648.276 1901.482 2341.715 3098.897 2059.736 2180.483 \n      17       18       19       20       21       22       23       24 \n3220.614 2687.877 2405.767 3170.552 3712.250 3041.984 1436.754 2585.173 \n      25       26       27       28       29       30       31       32 \n2984.581 2830.791 2399.264 1509.716 1775.081 1896.943 2634.975 3411.479 \n      33       34       35       36       37       38       39       40 \n1969.167 1376.645 2126.718 1713.278 2937.230 3386.286 2203.133 1649.541 \n      41       42       43       44       45       46       47       48 \n1984.038 2175.239 1860.417 3755.587 3170.453 3148.574 2088.211 2193.876 \n      49       50       51       52       53       54       55       56 \n3350.340 3253.971 2422.827 2771.180 2087.319 3427.878 2219.219 1757.332 \n      57       58       59       60       61       62       63       64 \n2766.498 2116.794 3056.978 2011.106 2952.359 1903.138 1427.501 3221.252 \n      65       66       67       68       69       70       71       72 \n1668.625 3050.249 3108.021 1451.672 2438.102 2193.159 2710.286 2762.986 \n      73       74       75       76       77       78       79       80 \n2212.214 2418.232 1805.806 1794.256 2880.903 3037.884 1483.078 2785.440 \n      81       82       83       84       85       86       87       88 \n2271.733 2790.511 1587.803 3276.106 2825.824 1720.098 1793.404 3048.518 \n      89       90 \n3263.245 3139.465 \n\n# The following line was provided by Dr. Handel. This can extract a single (the 1st one in this case) bootstrap sample into a data frame called dat_sample\ndat_sample = rsample::analysis(bootstrap_100$splits[[1]])\n\n# I then converted the list to matrix, so the following code can be ran.\npredictions_matrix &lt;- do.call(rbind, predictions_list)\n\n# The following code was based off of Dr. Handel's code\npredictions &lt;- predictions_matrix |&gt; apply(2, quantile,  c(0.055, 0.5, 0.945)) |&gt;  t()\n\nI then utilized ChatGPT to create the general gist of dataframe and plot below and editted it accordingly. I applied the observed values of the All_model to Observed and the predicted value to PointEstimate. The LowerBound, UpperBound, and the Median were all procured from the prediction list created from the function applied to the bootstrap.\n\n# Create a data frame containing the observed values and predictions\nplot_data &lt;- data.frame(\n  Observed = all_fit$Y,  # Observed values\n  PointEstimate = all_fit$.pred,  # Point estimate/ Original Prediction\n  Median = predictions[, 2],  # Median\n  LowerBound = predictions[, 1],  # Lower confidence bound\n  UpperBound = predictions[, 3]   # Upper confidence bound\n)\n\n# Create the plot with the X-axis being the original observed values on the x-axis and the point estimate on the y-axis. The original predictors are color coded in black. The median is in red, and the error bars(based on the UpperBound and LowerBound) or the confidence interval is in blue. \np4 &lt;- ggplot(plot_data, aes(x = Observed, y = PointEstimate)) +\n  geom_point(color = \"black\") +  # Point estimate/ original observed predictions\n  geom_errorbar(aes(ymin = LowerBound, ymax = UpperBound), width = 0.1, color = \"blue\") +  # Confidence interval\n  geom_point(aes(y = Median), color = \"red\") +  # Median\n  geom_abline(slope = 1, intercept = 0, linetype = \"dashed\", color = \"black\") +  # 45 degree line\n  labs(x = \"Observed Values\", y = \"Predicted Values\", title = \"Observed vs. Predicted Values for Model 2\") +\n  xlim(0, 6000) + ylim(0, 6000)+\n  theme_bw()  # Theme\n\n# Print the plot\nprint(p4)\n\n\n\n\nWhen observing the graph, there are a lot of overlaps between the prediction medians (sourced from the bootstrap samples) and the observed values (provided from the original fit). Despite it’s generally good fit, there are still a few signs of scattering, which might indicate other factors might still be influencing the data. Both the newly provided predicted medians and original follows the 45 degree angle line quite well. This shows that both that the predicted values and observed values from the original fit and the bootstrap fits are both fairly similar, which indicates a sign of a good model."
  },
  {
    "objectID": "tidytuesday-exercise/tidytuesday-exercise.html",
    "href": "tidytuesday-exercise/tidytuesday-exercise.html",
    "title": "Tidy Tuesday Exercise",
    "section": "",
    "text": "For this exercise, I will participate in the TidyTuesday data exploration and analysis. The data is gathered from the TidyTuesday Github repository and loaded as a package. I will wrangle and explore the data. The conduct model fitting and finally test the data."
  },
  {
    "objectID": "ml-models-exercise/ml-models-exercise.html",
    "href": "ml-models-exercise/ml-models-exercise.html",
    "title": "Machine Learning Exercise",
    "section": "",
    "text": "This analysis builds upon the fitting exercises conducted in Week 8 and Week 10. It utilizes the cleaned data, including the variable ‘RACE’ from the Week 8 exercise."
  },
  {
    "objectID": "ml-models-exercise/ml-models-exercise.html#lasso",
    "href": "ml-models-exercise/ml-models-exercise.html#lasso",
    "title": "Machine Learning Exercise",
    "section": "Lasso",
    "text": "Lasso\nFirst, we will start by tuning the Lasso model. I defined the range of penalty parameters to tune over. The range is from 1E-5 to 1E2. I picked 50 values linearly spaced on a log scale for tuning. To tune the grid using the workflow object, first it requires resampling with the entire data. Then the LASSO model will be tuned with the grid and the resamples prepared.\n\n# Define the LASSO tuning grid\nlasso_grid &lt;- expand.grid(penalty = 10^seq(-5, 2, length.out = 50))\n\nresamples_data &lt;- apparent(cleaned_mavo)\n\n#Setting up the model so that the tuning function (tune())can work\nLASSO_tunable &lt;- linear_reg(penalty = tune(), mixture = 1)%&gt;%\n  set_engine(\"glmnet\")%&gt;%\n  set_mode(\"regression\")\n\n\n#Define Lasso Model\nlasso_mavo2model &lt;- linear_reg(penalty = tune()) %&gt;%\n  set_engine(\"glmnet\")\n\n#workflow for tunable LASSO model\nmavo2_lassowf&lt;- workflow()%&gt;%\n  add_model(lasso_mavo2model)%&gt;%\n  add_recipe(mavo1_recipe) \n  \n# Tune Lasso model\nlasso_res &lt;- tune_grid(\n    workflow() %&gt;%\n    add_recipe(mavo1_recipe) %&gt;%\n    add_model(lasso_mavo2model),\n    resamples = resamples_data,\n    grid = lasso_grid,\n    metrics = metric_set(rmse),\n    control = control_grid(save_pred = TRUE)\n  )\n\nVisualizing the diagnostics from the LASSO tuning results to evaluate model performance.\n\np4 &lt;- autoplot(lasso_res)\n\np4\n\n\n\n\nThe diagnostic plot for the LASSO model indicates that it performs well when the penalty values are low. This is evident from the corresponding Root Mean Square Error (RMSE) metrics. However, as the penalty value increases, especially beyond a certain threshold, the model becomes more regularized. Consequently, some coefficients shrink to zero, introducing bias and leading to an increase in RMSE.\nInterestingly, in the initial stages of analysis, the LASSO model behaves similarly to a linear model when the penalty values are very low. Consequently, the RMSE values are comparable. This behavior occurs because at lower RMSE values, the model is less constrained by regularization."
  },
  {
    "objectID": "ml-models-exercise/ml-models-exercise.html#random-forest",
    "href": "ml-models-exercise/ml-models-exercise.html#random-forest",
    "title": "Machine Learning Exercise",
    "section": "Random Forest",
    "text": "Random Forest\nNext, I will tune the Random Forest model.\nFirst, I updated the model and workflow, setting the mumber of trees at 300. The tunning will focus on the parameters mtry and min_n, while all other parameters will remain at their default settings.\n\nrf_wf2 &lt;- rand_forest(mode = \"regression\", \n          mtry = tune(), \n          min_n = tune(), \n          trees = 300) %&gt;%\n      set_engine(\"ranger\")\n\nmavo2wf &lt;- workflow() %&gt;%\n    add_recipe(mavo1_recipe) %&gt;%\n    add_model(rf_wf2)\n\nI explored 7 x 7 parameter combinations by setting a tuning grid with the grid_regular() function, and setting the range for mtry from 1 to 7, and min_n from 1 to 21, with each parameter having levels.\n\n# Define by 7X7 grid\nrf_grid &lt;- grid_regular(mtry(range = c(1,7)),\n                        min_n(range = c(1,21)),\n                        levels = 7)\n# Tune model\nrf_res &lt;- mavo2wf %&gt;%\n          tune_grid(resamples = resamples_data,\n                    grid = rf_grid,\n                    metrics = metric_set(rmse))\n\nVisualizing the diagnostics Random Forest tuning results.\n\np5 &lt;- autoplot(rf_res)\n\np5\n\n\n\n\nThe diagnostic plot illustrates RMSE values across various tuning parameters, revealing that optimal results are achieved with higher number of randomly selected predictrs (mtry) values and lower minimal node size (min_n) values."
  },
  {
    "objectID": "ml-models-exercise/ml-models-exercise.html#tune-with-cv",
    "href": "ml-models-exercise/ml-models-exercise.html#tune-with-cv",
    "title": "Machine Learning Exercise",
    "section": "Tune with CV",
    "text": "Tune with CV\nNext, I will be tuning of both the LASSO and Random Forest (RF) models, using resampling with 5-fold cross-validation with 5 repititions. First, I set up the resample object, after which using the resamples to tune both models.\n\n#For reproducibility\nset.seed(4567)\n\n#Create resample object with cross-validation\nresamples_cv&lt;-vfold_cv(cleaned_mavo, v=5, repeats = 5)\n\n#Tunning the LASSO model\nlasso_cv &lt;- mavo2_lassowf %&gt;%\n  tune_grid(resamples = resamples_cv, grid = lasso_grid)\n\n#Tunning the RF model\nrf_cv &lt;- mavo2wf %&gt;%\n  tune_grid(resamples = resamples_cv, grid = rf_grid)\n\nPlotting the tuning results of LASSO and Random Forest models.\n\n# Analyze LASSO tuning results\np6 &lt;-autoplot(lasso_cv)\n\n# Analyze RF tuning results\np7 &lt;-autoplot(rf_cv)\n\np4+p5+p6+p7"
  },
  {
    "objectID": "tidytuesday-exercise/tidytuesday-exercise.html#load-packages",
    "href": "tidytuesday-exercise/tidytuesday-exercise.html#load-packages",
    "title": "Tidy Tuesday Exercise",
    "section": "Load Packages",
    "text": "Load Packages\n\nlibrary(broom)\nlibrary(ggplot2)\nlibrary(here)\n\nhere() starts at /Users/mutsa_n/Desktop/MADA-course/mutsanyamuranga-MADA-portfolio\n\nlibrary(dplyr)\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\nlibrary(purrr)\nlibrary(tidyr)\nlibrary(base)\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ lubridate 1.9.3     ✔ tibble    3.2.1\n✔ readr     2.1.5     \n\n\n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nlibrary(jsonlite)\n\n\nAttaching package: 'jsonlite'\n\nThe following object is masked from 'package:purrr':\n\n    flatten\n\nlibrary(janitor)\n\n\nAttaching package: 'janitor'\n\nThe following objects are masked from 'package:stats':\n\n    chisq.test, fisher.test\n\nlibrary(here)\nlibrary(fs)\nlibrary(lubridate)\nlibrary(dplyr)\nlibrary(tidymodels)\n\n── Attaching packages ────────────────────────────────────── tidymodels 1.1.1 ──\n✔ dials        1.2.1      ✔ rsample      1.2.0 \n✔ infer        1.0.6      ✔ tune         1.1.2 \n✔ modeldata    1.3.0      ✔ workflows    1.1.4 \n✔ parsnip      1.2.0      ✔ workflowsets 1.0.1 \n✔ recipes      1.0.10     ✔ yardstick    1.2.0 \n── Conflicts ───────────────────────────────────────── tidymodels_conflicts() ──\n✖ scales::discard()   masks purrr::discard()\n✖ dplyr::filter()     masks stats::filter()\n✖ recipes::fixed()    masks stringr::fixed()\n✖ jsonlite::flatten() masks purrr::flatten()\n✖ dplyr::lag()        masks stats::lag()\n✖ yardstick::spec()   masks readr::spec()\n✖ recipes::step()     masks stats::step()\n• Use tidymodels_prefer() to resolve common conflicts.\n\nlibrary(ranger)\nlibrary(yardstick)\nlibrary(leaflet)\nlibrary(maps)\n\n\nAttaching package: 'maps'\n\nThe following object is masked from 'package:purrr':\n\n    map"
  },
  {
    "objectID": "tidytuesday-exercise/tidytuesday-exercise.html#load-data",
    "href": "tidytuesday-exercise/tidytuesday-exercise.html#load-data",
    "title": "Tidy Tuesday Exercise",
    "section": "Load Data",
    "text": "Load Data\n\nlibrary(tidytuesdayR)\n\ntuesdata &lt;- tidytuesdayR::tt_load(2024, week = 15)\n\n\n    Downloading file 1 of 4: `eclipse_annular_2023.csv`\n    Downloading file 2 of 4: `eclipse_total_2024.csv`\n    Downloading file 3 of 4: `eclipse_partial_2023.csv`\n    Downloading file 4 of 4: `eclipse_partial_2024.csv`\n\neclipse_annular_2023 &lt;- tuesdata$eclipse_annular_2023\neclipse_total_2024 &lt;- tuesdata$eclipse_total_2024\neclipse_partial_2023 &lt;- tuesdata$eclipse_partial_2023\neclipse_partial_2024 &lt;- tuesdata$eclipse_partial_2024\n\neclipse_annular_2023 &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2024/2024-04-09/eclipse_annular_2023.csv')\neclipse_total_2024 &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2024/2024-04-09/eclipse_total_2024.csv')\neclipse_partial_2023 &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2024/2024-04-09/eclipse_partial_2023.csv')\neclipse_partial_2024 &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2024/2024-04-09/eclipse_partial_2024.csv')"
  },
  {
    "objectID": "tidytuesday-exercise/tidytuesday-exercise.html#data-structure",
    "href": "tidytuesday-exercise/tidytuesday-exercise.html#data-structure",
    "title": "Tidy Tuesday Exercise",
    "section": "Data Structure",
    "text": "Data Structure\n\n# Look at full summary of each data set\nsummary(eclipse_annular_2023)\n\n    state               name                lat             lon         \n Length:811         Length:811         Min.   :27.22   Min.   :-124.45  \n Class :character   Class :character   1st Qu.:31.30   1st Qu.:-111.98  \n Mode  :character   Mode  :character   Median :35.42   Median :-106.70  \n                                       Mean   :35.41   Mean   :-108.05  \n                                       3rd Qu.:38.42   3rd Qu.:-101.36  \n                                       Max.   :44.87   Max.   : -96.72  \n  eclipse_1         eclipse_2         eclipse_3         eclipse_4       \n Length:811        Length:811        Length:811        Length:811       \n Class1:hms        Class1:hms        Class1:hms        Class1:hms       \n Class2:difftime   Class2:difftime   Class2:difftime   Class2:difftime  \n Mode  :numeric    Mode  :numeric    Mode  :numeric    Mode  :numeric   \n                                                                        \n                                                                        \n  eclipse_5         eclipse_6       \n Length:811        Length:811       \n Class1:hms        Class1:hms       \n Class2:difftime   Class2:difftime  \n Mode  :numeric    Mode  :numeric   \n                                    \n                                    \n\nsummary(eclipse_partial_2023)\n\n    state               name                lat             lon         \n Length:31363       Length:31363       Min.   :17.96   Min.   :-176.60  \n Class :character   Class :character   1st Qu.:35.36   1st Qu.: -97.50  \n Mode  :character   Mode  :character   Median :39.56   Median : -89.26  \n                                       Mean   :38.80   Mean   : -91.97  \n                                       3rd Qu.:41.93   3rd Qu.: -81.14  \n                                       Max.   :71.25   Max.   : 174.11  \n  eclipse_1         eclipse_2         eclipse_3         eclipse_4       \n Length:31363      Length:31363      Length:31363      Length:31363     \n Class1:hms        Class1:hms        Class1:hms        Class1:hms       \n Class2:difftime   Class2:difftime   Class2:difftime   Class2:difftime  \n Mode  :numeric    Mode  :numeric    Mode  :numeric    Mode  :numeric   \n                                                                        \n                                                                        \n  eclipse_5       \n Length:31363     \n Class1:hms       \n Class2:difftime  \n Mode  :numeric   \n                  \n                  \n\nsummary(eclipse_partial_2024)\n\n    state               name                lat             lon         \n Length:28844       Length:28844       Min.   :17.96   Min.   :-176.60  \n Class :character   Class :character   1st Qu.:35.24   1st Qu.: -99.08  \n Mode  :character   Mode  :character   Median :39.52   Median : -90.30  \n                                       Mean   :38.76   Mean   : -93.00  \n                                       3rd Qu.:42.04   3rd Qu.: -81.16  \n                                       Max.   :71.25   Max.   : 174.11  \n  eclipse_1         eclipse_2         eclipse_3         eclipse_4       \n Length:28844      Length:28844      Length:28844      Length:28844     \n Class1:hms        Class1:hms        Class1:hms        Class1:hms       \n Class2:difftime   Class2:difftime   Class2:difftime   Class2:difftime  \n Mode  :numeric    Mode  :numeric    Mode  :numeric    Mode  :numeric   \n                                                                        \n                                                                        \n  eclipse_5       \n Length:28844     \n Class1:hms       \n Class2:difftime  \n Mode  :numeric   \n                  \n                  \n\nsummary(eclipse_total_2024)\n\n    state               name                lat             lon         \n Length:3330        Length:3330        Min.   :28.45   Min.   :-101.16  \n Class :character   Class :character   1st Qu.:35.42   1st Qu.: -92.41  \n Mode  :character   Mode  :character   Median :39.24   Median : -86.56  \n                                       Mean   :38.33   Mean   : -86.93  \n                                       3rd Qu.:41.22   3rd Qu.: -82.31  \n                                       Max.   :46.91   Max.   : -67.43  \n  eclipse_1         eclipse_2         eclipse_3         eclipse_4       \n Length:3330       Length:3330       Length:3330       Length:3330      \n Class1:hms        Class1:hms        Class1:hms        Class1:hms       \n Class2:difftime   Class2:difftime   Class2:difftime   Class2:difftime  \n Mode  :numeric    Mode  :numeric    Mode  :numeric    Mode  :numeric   \n                                                                        \n                                                                        \n  eclipse_5         eclipse_6       \n Length:3330       Length:3330      \n Class1:hms        Class1:hms       \n Class2:difftime   Class2:difftime  \n Mode  :numeric    Mode  :numeric   \n                                    \n                                    \n\n# Look at names of variables\nnames(eclipse_annular_2023)\n\n [1] \"state\"     \"name\"      \"lat\"       \"lon\"       \"eclipse_1\" \"eclipse_2\"\n [7] \"eclipse_3\" \"eclipse_4\" \"eclipse_5\" \"eclipse_6\"\n\nnames(eclipse_partial_2023)\n\n[1] \"state\"     \"name\"      \"lat\"       \"lon\"       \"eclipse_1\" \"eclipse_2\"\n[7] \"eclipse_3\" \"eclipse_4\" \"eclipse_5\"\n\nnames(eclipse_partial_2024)\n\n[1] \"state\"     \"name\"      \"lat\"       \"lon\"       \"eclipse_1\" \"eclipse_2\"\n[7] \"eclipse_3\" \"eclipse_4\" \"eclipse_5\"\n\nnames(eclipse_total_2024)\n\n [1] \"state\"     \"name\"      \"lat\"       \"lon\"       \"eclipse_1\" \"eclipse_2\"\n [7] \"eclipse_3\" \"eclipse_4\" \"eclipse_5\" \"eclipse_6\"\n\n# Look at structure of data sets\nstr(eclipse_annular_2023)\n\nspc_tbl_ [811 × 10] (S3: spec_tbl_df/tbl_df/tbl/data.frame)\n $ state    : chr [1:811] \"AZ\" \"AZ\" \"AZ\" \"AZ\" ...\n $ name     : chr [1:811] \"Chilchinbito\" \"Chinle\" \"Del Muerto\" \"Dennehotso\" ...\n $ lat      : num [1:811] 36.5 36.2 36.2 36.8 35.7 ...\n $ lon      : num [1:811] -110 -110 -109 -110 -109 ...\n $ eclipse_1: 'hms' num [1:811] 15:10:50 15:11:10 15:11:20 15:10:50 ...\n  ..- attr(*, \"units\")= chr \"secs\"\n $ eclipse_2: 'hms' num [1:811] 15:56:20 15:56:50 15:57:00 15:56:20 ...\n  ..- attr(*, \"units\")= chr \"secs\"\n $ eclipse_3: 'hms' num [1:811] 16:30:29 16:31:21 16:31:13 16:29:50 ...\n  ..- attr(*, \"units\")= chr \"secs\"\n $ eclipse_4: 'hms' num [1:811] 16:33:31 16:34:06 16:34:31 16:34:07 ...\n  ..- attr(*, \"units\")= chr \"secs\"\n $ eclipse_5: 'hms' num [1:811] 17:09:40 17:10:30 17:10:40 17:09:40 ...\n  ..- attr(*, \"units\")= chr \"secs\"\n $ eclipse_6: 'hms' num [1:811] 18:02:10 18:03:20 18:03:30 18:02:00 ...\n  ..- attr(*, \"units\")= chr \"secs\"\n - attr(*, \"spec\")=\n  .. cols(\n  ..   state = col_character(),\n  ..   name = col_character(),\n  ..   lat = col_double(),\n  ..   lon = col_double(),\n  ..   eclipse_1 = col_time(format = \"\"),\n  ..   eclipse_2 = col_time(format = \"\"),\n  ..   eclipse_3 = col_time(format = \"\"),\n  ..   eclipse_4 = col_time(format = \"\"),\n  ..   eclipse_5 = col_time(format = \"\"),\n  ..   eclipse_6 = col_time(format = \"\")\n  .. )\n - attr(*, \"problems\")=&lt;externalptr&gt; \n\nstr(eclipse_partial_2023)\n\nspc_tbl_ [31,363 × 9] (S3: spec_tbl_df/tbl_df/tbl/data.frame)\n $ state    : chr [1:31363] \"AL\" \"AL\" \"AL\" \"AL\" ...\n $ name     : chr [1:31363] \"Abanda\" \"Abbeville\" \"Adamsville\" \"Addison\" ...\n $ lat      : num [1:31363] 33.1 31.6 33.6 34.2 32.9 ...\n $ lon      : num [1:31363] -85.5 -85.3 -87 -87.2 -87.7 ...\n $ eclipse_1: 'hms' num [1:31363] 15:41:20 15:42:30 15:38:20 15:37:50 ...\n  ..- attr(*, \"units\")= chr \"secs\"\n $ eclipse_2: 'hms' num [1:31363] 16:23:30 16:25:50 16:20:50 16:19:50 ...\n  ..- attr(*, \"units\")= chr \"secs\"\n $ eclipse_3: 'hms' num [1:31363] 17:11:10 17:13:50 17:07:50 17:06:50 ...\n  ..- attr(*, \"units\")= chr \"secs\"\n $ eclipse_4: 'hms' num [1:31363] 18:00:00 18:03:10 17:56:30 17:55:10 ...\n  ..- attr(*, \"units\")= chr \"secs\"\n $ eclipse_5: 'hms' num [1:31363] 18:45:10 18:49:30 18:42:10 18:40:30 ...\n  ..- attr(*, \"units\")= chr \"secs\"\n - attr(*, \"spec\")=\n  .. cols(\n  ..   state = col_character(),\n  ..   name = col_character(),\n  ..   lat = col_double(),\n  ..   lon = col_double(),\n  ..   eclipse_1 = col_time(format = \"\"),\n  ..   eclipse_2 = col_time(format = \"\"),\n  ..   eclipse_3 = col_time(format = \"\"),\n  ..   eclipse_4 = col_time(format = \"\"),\n  ..   eclipse_5 = col_time(format = \"\")\n  .. )\n - attr(*, \"problems\")=&lt;externalptr&gt; \n\nstr(eclipse_partial_2024)\n\nspc_tbl_ [28,844 × 9] (S3: spec_tbl_df/tbl_df/tbl/data.frame)\n $ state    : chr [1:28844] \"AL\" \"AL\" \"AL\" \"AL\" ...\n $ name     : chr [1:28844] \"Abanda\" \"Abbeville\" \"Adamsville\" \"Addison\" ...\n $ lat      : num [1:28844] 33.1 31.6 33.6 34.2 32.9 ...\n $ lon      : num [1:28844] -85.5 -85.3 -87 -87.2 -87.7 ...\n $ eclipse_1: 'hms' num [1:28844] 17:43:00 17:41:40 17:41:00 17:41:30 ...\n  ..- attr(*, \"units\")= chr \"secs\"\n $ eclipse_2: 'hms' num [1:28844] 18:24:10 18:21:40 18:23:10 18:24:10 ...\n  ..- attr(*, \"units\")= chr \"secs\"\n $ eclipse_3: 'hms' num [1:28844] 19:02:00 19:00:30 19:00:00 19:00:30 ...\n  ..- attr(*, \"units\")= chr \"secs\"\n $ eclipse_4: 'hms' num [1:28844] 19:39:20 19:38:50 19:36:40 19:36:40 ...\n  ..- attr(*, \"units\")= chr \"secs\"\n $ eclipse_5: 'hms' num [1:28844] 20:18:50 20:17:20 20:17:30 20:18:00 ...\n  ..- attr(*, \"units\")= chr \"secs\"\n - attr(*, \"spec\")=\n  .. cols(\n  ..   state = col_character(),\n  ..   name = col_character(),\n  ..   lat = col_double(),\n  ..   lon = col_double(),\n  ..   eclipse_1 = col_time(format = \"\"),\n  ..   eclipse_2 = col_time(format = \"\"),\n  ..   eclipse_3 = col_time(format = \"\"),\n  ..   eclipse_4 = col_time(format = \"\"),\n  ..   eclipse_5 = col_time(format = \"\")\n  .. )\n - attr(*, \"problems\")=&lt;externalptr&gt; \n\nstr(eclipse_total_2024)\n\nspc_tbl_ [3,330 × 10] (S3: spec_tbl_df/tbl_df/tbl/data.frame)\n $ state    : chr [1:3330] \"AR\" \"AR\" \"AR\" \"AR\" ...\n $ name     : chr [1:3330] \"Acorn\" \"Adona\" \"Alexander\" \"Alicia\" ...\n $ lat      : num [1:3330] 34.6 35 34.6 35.9 35.4 ...\n $ lon      : num [1:3330] -94.2 -92.9 -92.5 -91.1 -93.7 ...\n $ eclipse_1: 'hms' num [1:3330] 17:30:40 17:33:20 17:33:20 17:37:30 ...\n  ..- attr(*, \"units\")= chr \"secs\"\n $ eclipse_2: 'hms' num [1:3330] 18:15:50 18:18:30 18:18:30 18:22:40 ...\n  ..- attr(*, \"units\")= chr \"secs\"\n $ eclipse_3: 'hms' num [1:3330] 18:47:35 18:50:08 18:51:09 18:54:29 ...\n  ..- attr(*, \"units\")= chr \"secs\"\n $ eclipse_4: 'hms' num [1:3330] 18:51:37 18:54:22 18:53:38 18:58:05 ...\n  ..- attr(*, \"units\")= chr \"secs\"\n $ eclipse_5: 'hms' num [1:3330] 19:23:40 19:26:10 19:26:20 19:29:50 ...\n  ..- attr(*, \"units\")= chr \"secs\"\n $ eclipse_6: 'hms' num [1:3330] 20:08:30 20:10:50 20:11:10 20:14:10 ...\n  ..- attr(*, \"units\")= chr \"secs\"\n - attr(*, \"spec\")=\n  .. cols(\n  ..   state = col_character(),\n  ..   name = col_character(),\n  ..   lat = col_double(),\n  ..   lon = col_double(),\n  ..   eclipse_1 = col_time(format = \"\"),\n  ..   eclipse_2 = col_time(format = \"\"),\n  ..   eclipse_3 = col_time(format = \"\"),\n  ..   eclipse_4 = col_time(format = \"\"),\n  ..   eclipse_5 = col_time(format = \"\"),\n  ..   eclipse_6 = col_time(format = \"\")\n  .. )\n - attr(*, \"problems\")=&lt;externalptr&gt; \n\n#Look at number of rows and columns in each data set\nnrow(eclipse_annular_2023)\n\n[1] 811\n\nnrow(eclipse_partial_2023)\n\n[1] 31363\n\nnrow(eclipse_partial_2024)\n\n[1] 28844\n\nnrow(eclipse_total_2024)\n\n[1] 3330\n\nncol(eclipse_annular_2023)\n\n[1] 10\n\nncol(eclipse_partial_2023)\n\n[1] 9\n\nncol(eclipse_partial_2024)\n\n[1] 9\n\nncol(eclipse_total_2024)\n\n[1] 10\n\n\nFrom the first looks, we see that there are four data sets with two that contain data from an annular eclipse in 2023 and two that contain data from a total eclipse in 2024. The variables captured are the location of the eclipse and time of day, capturing the state, name of the city, longitude and latitude. The five or six eclipse variables are the time of day at which the which the moon contacts the sun at the location at various points of the eclipse. For example, eclipse_3 is time at which annularity begins in this location in 2023 and time at which totality begins in this location in 2024.\n\nFeature Engineering\nI added a column for duration of visibility in minutes for all solar eclipses from first to last contact.\n\n# Duration of the eclipse to total eclipse of 2024\neclipse_total_2024&lt;- eclipse_total_2024 %&gt;%\n  mutate(\n    eclipse_1_time = hms(eclipse_1),\n    eclipse_6_time = hms(eclipse_6),\n    duration = as.numeric(eclipse_6_time - eclipse_1_time)/60 )\n\n# Duration of the eclipse to annular eclipse of 2023\neclipse_annular_2023&lt;- eclipse_annular_2023 %&gt;%\n  mutate(\n    eclipse_1_time = hms(eclipse_1),\n    eclipse_6_time = hms(eclipse_6),\n    duration = as.numeric(eclipse_6_time - eclipse_1_time)/60 )\n\n# Duration of the eclipse to partial eclipse of 2024\neclipse_partial_2024&lt;- eclipse_partial_2024 %&gt;%\n  mutate(\n    eclipse_1_time = hms(eclipse_1),\n    eclipse_5_time = hms(eclipse_5),\n    duration = as.numeric(eclipse_5_time - eclipse_1_time)/60 )\n\n# Duration of the eclipse to the partial eclipse of 2024\neclipse_partial_2023&lt;- eclipse_partial_2023 %&gt;%\n  mutate(\n    eclipse_1_time = hms(eclipse_1),\n    eclipse_5_time = hms(eclipse_5),\n    duration = as.numeric(eclipse_5_time - eclipse_1_time)/60 )\n\nI will add a column of eclipse year in each of the data sets so that each observation can be identified as to which year the eclipse was from and also another column of eclipse type for the purpose of plotting.\n\n# Identifier to the Total Eclipse 2024 data \neclipse_total_2024 &lt;- mutate(eclipse_total_2024, eclipse_type='Total_2024', eclipse_year='2024')\n\n# Identifier  to the Annular Eclipse 2023 data \neclipse_annular_2023 &lt;- mutate(eclipse_annular_2023, eclipse_type='Annular_2023', eclipse_year = '2023')\n\n# Identifier to the Partial Eclipse 2024 data \neclipse_partial_2024 &lt;- mutate(eclipse_partial_2024, eclipse_type='Partial_2024',eclipse_year='2024')\n\n# Identifier to the Partial Eclipse 2023 data \neclipse_partial_2023 &lt;- mutate(eclipse_partial_2023, eclipse_type='Partial_2023',eclipse_year='2023')\n\nNow, I will merge all of the datasets into one data set with all the 4 datasets by rows and kept state, city name, lattitude, longitude, duration and eclipse year in the final data. I converted the eclipse_year to a factor variable.\n\n#Combining all the data sets by row\neclipse_all&lt;- bind_rows(eclipse_total_2024, eclipse_annular_2023, eclipse_partial_2024,eclipse_partial_2023 )%&gt;%\n  #Selecting relevant columns\n  select(state, name, lat, lon, duration, eclipse_year, eclipse_type)%&gt;%\n  #convert to factor\n  mutate(eclipse_year=factor(eclipse_year))"
  },
  {
    "objectID": "tidytuesday-exercise/tidytuesday-exercise.html#define-each-model",
    "href": "tidytuesday-exercise/tidytuesday-exercise.html#define-each-model",
    "title": "Tidy Tuesday Exercise",
    "section": "Define Each Model",
    "text": "Define Each Model\n\n# Linear Model\neclin_model &lt;- linear_reg()%&gt;%\n  set_engine (\"lm\")%&gt;%\n  set_mode(\"regression\")\n\n# Random Forest Model\neclforest_model &lt;- rand_forest()%&gt;%\n  set_engine(\"ranger\", seed = rngseed)%&gt;%\n  set_mode(\"regression\")\n\n# Decision Tree Model\necltree_model &lt;- decision_tree()%&gt;%\n  set_engine(\"rpart\")%&gt;%\n  set_mode(\"regression\")\n\n\nWorkflow For All Models\n\n# Workflow for Linear Model\neclin_wf &lt;- workflow()%&gt;%\n  add_model(eclin_model)%&gt;%\n  add_formula(duration ~ eclipse_year)\n\n# Workflow for Random Forest Model\neclforest_wf &lt;- workflow()%&gt;%\n  add_model(eclforest_model)%&gt;%\n  add_formula(duration ~ eclipse_year)\n\n# Workflow for Decision Tree Model\necltree_wf &lt;- workflow()%&gt;%\n  add_model(ecltree_model)%&gt;%\n  add_formula(duration ~ eclipse_year)"
  },
  {
    "objectID": "tidytuesday-exercise/tidytuesday-exercise.html#fit-the-models",
    "href": "tidytuesday-exercise/tidytuesday-exercise.html#fit-the-models",
    "title": "Tidy Tuesday Exercise",
    "section": "Fit The Models",
    "text": "Fit The Models\n\n# Define Resampling Control\nresampling_control &lt;- control_resamples(save_pred = TRUE)\n\n# Linear Fit with CV\neclinfit_cv &lt;- fit_resamples(eclin_wf, resamples=cv_fold, metrics = metric_set(rmse, rsq),control = resampling_control)\n\n# Random forest Fit with CV\neclforestfit_cv &lt;- fit_resamples (eclforest_wf, resamples = cv_fold, metrics = metric_set(rmse, rsq),control = resampling_control)\n\n# Decision Tree Fit with CV\necltreefit_cv &lt;- fit_resamples(ecltree_wf, resamples = cv_fold, metrics = metric_set(rmse, rsq),control = resampling_control)\n\n#Collecting Metrics\nlin_metrics &lt;-collect_metrics(eclinfit_cv)\nRF_metrics &lt;-collect_metrics(eclforestfit_cv)\nDT_metrics &lt;-collect_metrics(ecltreefit_cv)\nLin_predicts&lt;- collect_predictions(eclinfit_cv)\nForest_predicts &lt;- collect_predictions(eclforestfit_cv)\nTree_predicts &lt;- collect_predictions(ecltreefit_cv)\n\nI will choose the best model by comparing the RMSE metric, accuracy of the predicted vs observed value and residuals of the models.\n\nMetrics\n\n# Mean RMSE and R² for Each Model\n\n## Linear\nlin_rmse &lt;- lin_metrics %&gt;% filter(.metric == \"rmse\") %&gt;% pull(mean)\nlin_rsq &lt;- lin_metrics %&gt;% filter(.metric == \"rsq\") %&gt;% pull(mean)\n\n## Random Forest\nRF_rmse &lt;- RF_metrics %&gt;% filter(.metric == \"rmse\") %&gt;% pull(mean)\nRF_rsq &lt;- RF_metrics %&gt;% filter(.metric == \"rsq\") %&gt;% pull(mean)\n\n## Decision Tree\nDT_rmse &lt;- DT_metrics %&gt;% filter(.metric == \"rmse\") %&gt;% pull(mean)\nDT_rsq &lt;- DT_metrics %&gt;% filter(.metric == \"rsq\") %&gt;% pull(mean)\n\n## Metrics\ncat(\"Linear Regression - RMSE:\", lin_rmse, \"R²:\", lin_rsq, \"\\n\")\n\nLinear Regression - RMSE: 21.18793 R²: 0.1960076 \n\ncat(\"Random Forest - RMSE:\", RF_rmse, \"R²:\", RF_rsq, \"\\n\")\n\nRandom Forest - RMSE: 21.18795 R²: 0.1960076 \n\ncat(\"Decision Tree - RMSE:\", DT_rmse, \"R²:\", DT_rsq, \"\\n\")\n\nDecision Tree - RMSE: 21.18793 R²: 0.1960076 \n\n\nThe linear and decision tree models perform marginally better than the random forest model based on the RMSE metric though the values are approximately similar.\n\n\nPredictions\nBelow are the predicted values for the training data and residuals.\n\n#Predicting on the training data\nLin_predicts&lt;- collect_predictions(eclinfit_cv)\nForest_predicts &lt;- collect_predictions(eclforestfit_cv)\nTree_predicts &lt;- collect_predictions(ecltreefit_cv)\n\n# Calculate Residuals\nLin_predicts &lt;- Lin_predicts %&gt;% mutate(residuals = .pred - duration)\nForest_predicts &lt;- Forest_predicts %&gt;% mutate(residuals = .pred - duration)\nTree_predicts &lt;- Tree_predicts %&gt;% mutate(residuals = .pred - duration)\n\n# Labeling each data frame of predictions before combining\nLin_predicts$model &lt;- \"Linear Regression\"\nForest_predicts$model &lt;- \"Random Forest\"\nTree_predicts$model &lt;- \"Decision Tree\"\n\n# Combine all predictions into one dataframe\ncombine_predicts &lt;- bind_rows(Lin_predicts, Forest_predicts, Tree_predicts)"
  },
  {
    "objectID": "tidytuesday-exercise/tidytuesday-exercise.html#plotting",
    "href": "tidytuesday-exercise/tidytuesday-exercise.html#plotting",
    "title": "Tidy Tuesday Exercise",
    "section": "Plotting",
    "text": "Plotting\nThere are two plots:\nPredicted vs Observed Values Plot A Residuals Plot.\n\n# Predicted vs. Observed plot\nggplot(combine_predicts, aes(x = duration, y = .pred, color = model)) +\n  geom_point(alpha = 0.6) +  \n  geom_abline(slope = 1, intercept = 0, linetype = \"dashed\", color = \"black\") +  # 45-degree line\n  labs(\n    title = \"Predicted vs. Observed Values\",\n    x = \"Observed\",\n    y = \"Predicted\"\n  ) +\n  theme_minimal() +\n  scale_color_manual(values = c(\"Decision Tree\" = \"orange\", \"Random Forest\" = \"purple\", \"Linear Regression\" = \"blue\"))\n\n\n\n# Residual Plot\nggplot(combine_predicts, aes(x = duration, y = residuals, color = model)) +\n  geom_point(alpha = 0.6) +  # Adjust opacity with alpha if needed\n  geom_hline(yintercept = 0,linetype = \"dashed\", color = \"black\") +  \n  labs(\n    title = \"Residual Plot\",\n    x = \"Observed\",\n    y = \"Residuals\"\n  ) +\n  theme_minimal() +\n  scale_color_manual(values = c(\"Decision Tree\" = \"red\", \"Random Forest\" = \"violet\", \"Linear Regression\" = \"lightblue\"))\n\n\n\n\nThe models failed to capture the predictor-response relationship, despite overlapping predictions and residuals indicated by the plots. I retained the linear model and extracted coefficients to interpret the eclipse year’s impact on duration"
  },
  {
    "objectID": "tidytuesday-exercise/tidytuesday-exercise.html#model-selection",
    "href": "tidytuesday-exercise/tidytuesday-exercise.html#model-selection",
    "title": "Tidy Tuesday Exercise",
    "section": "Model Selection",
    "text": "Model Selection\n\n# Coefficients of Fitted Model\neclin_fit &lt;- fit(eclin_wf, data=eclipstrain_data)\neclin_est &lt;- extract_fit_parsnip(eclin_fit)\ntidy(eclin_est)\n\n# A tibble: 2 × 5\n  term             estimate std.error statistic p.value\n  &lt;chr&gt;               &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt;\n1 (Intercept)         165.      0.132     1253.       0\n2 eclipse_year2024    -20.9     0.187     -112.       0\n\n\nThe linear model indicates that, on average, eclipse durations in 2024 were shorter than those in 2023. Next, I’ll apply this model to the test data for predictions and performance evaluation. The model was trained on the complete training data set before making predictions on the test data.\n\nTraining\n\n# Train the Workflow on Training Data\neclin_final &lt;- fit(eclin_wf, eclipstrain_data)\n\n# Make Predictions on Test Data\necltest_predicts &lt;- predict(eclin_final, new_data = eclipstest_data)\n\n# Combine Predictions to Test data\neclipstest_data &lt;- eclipstest_data %&gt;%\n  bind_cols(ecltest_predicts) \n\n# Calculate Residuals\neclipstest_data &lt;- eclipstest_data%&gt;%mutate(residuals= .pred - duration)\n\n# Calculate Performance Metrics\neclipstest_data %&gt;%\n  metrics(truth = duration, estimate = .pred)\n\n# A tibble: 3 × 3\n  .metric .estimator .estimate\n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;\n1 rmse    standard      20.9  \n2 rsq     standard       0.197\n3 mae     standard      11.9  \n\n\nAs we can see the RMSE metric of the model has a slightly higher value for the test data compared to the training data.\n\n\nFinal Plotting of Tested Data\n\n# Observed vs. predicted plot\nggplot(eclipstest_data, aes(x = duration, y = .pred)) +\n  geom_point(alpha = 0.6) +  \n  geom_abline(slope = 1, intercept = 0, linetype = \"dashed\", color = \"black\") +  # 45-degree line\n  labs(\n    title = \"Predicted vs. Observed Values\",\n    x = \"Observed\",\n    y = \"Predicted\"\n  ) +\n  theme_minimal()\n\n\n\n#Residual plot\nggplot(eclipstest_data, aes(x = duration, y = residuals)) +\n  geom_point(alpha = 0.6) +  # Adjust opacity with alpha if needed\n  geom_hline(yintercept = 0, linetype = \"dashed\", color = \"black\") +  \n  labs(\n    title = \"Residual Plot\",\n    x = \"Observed\",\n    y = \"Residuals\"\n  ) +\n  theme_minimal()"
  },
  {
    "objectID": "tidytuesday-exercise/tidytuesday-exercise.html#conclusion",
    "href": "tidytuesday-exercise/tidytuesday-exercise.html#conclusion",
    "title": "Tidy Tuesday Exercise",
    "section": "Conclusion",
    "text": "Conclusion\nThe Residual plot and the Predicted vs. Observed plots for the test set exhibited similar patterns to those observed in the training set. This consistency suggests that the model performs consistently on both sets and is unlikely to be over-fitting to the training data. However, the presence of systematic patterns in the residuals indicates that the model struggles to capture the underlying complexity of the data.\nThe analysis aimed to model eclipse duration based on the year, using data from the 2023 and 2024 eclipses sourced from the Tidy Tuesday GitHub repository. Initially, the hypothesis was that eclipses in 2023 had longer durations than those in 2024, based on preliminary exploratory data analysis. I compared Linear Regression, Random Forest, and Decision Tree models using cross-validation. Residuals from all models exhibited distinct patterns, suggesting that none of the models adequately captured the data’s complexity. Despite this, I opted for Linear Regression due to its simplicity and interpret-ability. The chosen model supported the hypothesis that 2023 eclipses had longer durations than 2024 eclipses. While the model performed consistently on both training and test data, its limitations were evident in residual patterns. To enhance model fit, future analysis could explore non-linear models or incorporate additional variables."
  }
]